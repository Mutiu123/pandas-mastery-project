{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34d4cac",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Root logger = CEO**  \n",
    "   Configure it once, everyone inherits\n",
    "\n",
    "2. **Log levels = audience**  \n",
    "   - DEBUG → developers (internal minutiae)\n",
    "   - INFO → operators (milestones)\n",
    "   - WARNING → when you fix something automatically\n",
    "   - ERROR → when you can't fix something\n",
    "   - CRITICAL → pipeline must stop\n",
    "\n",
    "3. **Handlers = destinations**  \n",
    "   - Console (real-time)\n",
    "   - Full log file (forensic)\n",
    "   - Error file (quick triage)\n",
    "\n",
    "4. **Formatters = presentation**  \n",
    "   - Detailed for files (who, what, where, when)\n",
    "   - Simple for console (just the message)\n",
    "\n",
    "5. **Rotation = safety**  \n",
    "   - Keeps disk from filling up\n",
    "   - RotatingFileHandler handles it automatically\n",
    "\n",
    "**In one sentence:**  \n",
    "*setup_logging() creates a central logging hub with multiple outputs, each curated for its specific audience.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daabdea",
   "metadata": {},
   "source": [
    "## Summary: The Complete Picture\n",
    "\n",
    "Here's what `setup_logging()` does in ONE DIAGRAM:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ ROOT LOGGER (central headquarters)                          │\n",
    "│ - Level: DEBUG (catch everything)                           │\n",
    "│ - Has 3 handlers attached:                                  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "         |\n",
    "    ┌────────────────────────────────────────────┐\n",
    "    │ Every logger created via                   │\n",
    "    │ logging.getLogger(__name__)                │\n",
    "    │ automatically inherits these 3 handlers    │\n",
    "    └────────────────────────────────────────────┘\n",
    "     |                  |                  |\n",
    "┌─────────────┐  ┌────────────────┐  ┌──────────────┐\n",
    "│  CONSOLE    │  │  pipeline.log  │  │  errors.log  │\n",
    "│ (INFO+)     │  │  (DEBUG+)      │  │  (ERROR+)    │\n",
    "│             │  │                │  │              │\n",
    "│ Operators   │  │ Developers     │  │ On-call      │\n",
    "│ see:        │  │ see:           │  │ engineers    │\n",
    "│ Y INFO      │  │ Y DEBUG        │  │ see:         │\n",
    "│ Y WARNING   │  │ Y INFO         │  │ Y ERROR      │\n",
    "│ Y ERROR     │  │ Y WARNING      │  │ Y CRITICAL   │\n",
    "│ Y CRITICAL  │  │ Y ERROR        │  │              │\n",
    "│ N DEBUG     │  │ Y CRITICAL     │  │ N DEBUG      │\n",
    "│             │  │ (rotates @ 5MB)│  │ (rotates)    │\n",
    "└─────────────┘  └────────────────┘  └──────────────┘\n",
    "```\n",
    "\n",
    "**The genius:** \n",
    "- One function call sets everything up\n",
    "- Different audiences get different detail levels\n",
    "- Log files don't fill the disk\n",
    "- Developer can trace any issue with full DEBUG info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436a8911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENTS OF pipeline.log (DEBUG+):\n",
      "============================================================\n",
      "File not found\n",
      "\n",
      "CONTENTS OF errors.log (ERROR+):\n",
      "============================================================\n",
      "File not found\n",
      "\n",
      "Observations:\n",
      "   - pipeline.log HAS all 5 messages (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
      "   - errors.log HAS ONLY 2 messages (ERROR and CRITICAL)\n",
      "   - Console (above) didn't show DEBUG\n",
      "\n",
      "This proves the filtering works correctly!\n"
     ]
    }
   ],
   "source": [
    "# Check what was actually written to the files\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Define project root (this cell needs to be self-contained)\n",
    "project_root = r\"c:\\Users\\adegb\\Desktop\\python-logging-mastery\"\n",
    "\n",
    "test_log_dir = os.path.join(project_root, \"test_logs\")\n",
    "pipeline_log = os.path.join(test_log_dir, \"pipeline.log\")\n",
    "errors_log = os.path.join(test_log_dir, \"errors.log\")\n",
    "\n",
    "print(\"CONTENTS OF pipeline.log (DEBUG+):\")\n",
    "print(\"=\" * 60)\n",
    "if os.path.exists(pipeline_log):\n",
    "    with open(pipeline_log, 'r') as f:\n",
    "        content = f.read()\n",
    "        # Show last 5 lines (the ones we just wrote)\n",
    "        lines = content.strip().split('\\n')\n",
    "        for line in lines[-5:]:\n",
    "            print(line)\n",
    "else:\n",
    "    print(\"File not found\")\n",
    "\n",
    "print()\n",
    "print(\"CONTENTS OF errors.log (ERROR+):\")\n",
    "print(\"=\" * 60)\n",
    "if os.path.exists(errors_log):\n",
    "    with open(errors_log, 'r') as f:\n",
    "        content = f.read()\n",
    "        lines = content.strip().split('\\n')\n",
    "        for line in lines[-5:]:\n",
    "            print(line)\n",
    "else:\n",
    "    print(\"File not found\")\n",
    "\n",
    "print()\n",
    "print(\"Observations:\")\n",
    "print(\"   - pipeline.log HAS all 5 messages (DEBUG, INFO, WARNING, ERROR, CRITICAL)\")\n",
    "print(\"   - errors.log HAS ONLY 2 messages (ERROR and CRITICAL)\")\n",
    "print(\"   - Console (above) didn't show DEBUG\")\n",
    "print()\n",
    "print(\"This proves the filtering works correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2333767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a WARNING message\n",
      "This is an ERROR message\n",
      "This is a CRITICAL message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging test messages...\n",
      "\n",
      "Messages sent! Now let's check what was written to files...\n"
     ]
    }
   ],
   "source": [
    "# Now let's log at different levels and check what appears where\n",
    "import logging\n",
    "\n",
    "# Create a named logger (like etl/extractor.py does)\n",
    "logger = logging.getLogger(\"etl.extractor\")\n",
    "\n",
    "print(\"Logging test messages...\")\n",
    "print()\n",
    "\n",
    "logger.debug(\"This is a DEBUG message\")\n",
    "logger.info(\"This is an INFO message\")\n",
    "logger.warning(\"This is a WARNING message\")\n",
    "logger.error(\"This is an ERROR message\")\n",
    "logger.critical(\"This is a CRITICAL message\")\n",
    "\n",
    "print(\"Messages sent! Now let's check what was written to files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da771164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:31:53 | INFO     | Logging system initialized -- console=INFO, file=DEBUG, errors=ERROR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging system initialized!\n",
      "   Log directory: c:\\Users\\adegb\\Desktop\\python-logging-mastery\\test_logs\n",
      "\n",
      "Root logger has 3 handlers:\n",
      "   1. StreamHandler @ level 20\n",
      "   2. RotatingFileHandler @ level 10\n",
      "      File: c:\\Users\\adegb\\Desktop\\python-logging-mastery\\test_logs\\pipeline.log\n",
      "   3. RotatingFileHandler @ level 40\n",
      "      File: c:\\Users\\adegb\\Desktop\\python-logging-mastery\\test_logs\\errors.log\n"
     ]
    }
   ],
   "source": [
    "# Import from the actual project\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = r\"c:\\Users\\adegb\\Desktop\\python-logging-mastery\"\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import and run the actual setup\n",
    "from config.logging_config import setup_logging\n",
    "import logging\n",
    "\n",
    "# Setup logging (creates the handlers, formatters, etc.)\n",
    "test_log_dir = os.path.join(project_root, \"test_logs\")\n",
    "setup_logging(log_dir=test_log_dir)\n",
    "\n",
    "print(\"Logging system initialized!\")\n",
    "print(f\"   Log directory: {test_log_dir}\")\n",
    "print()\n",
    "\n",
    "# Get root logger to inspect what was added\n",
    "root_logger = logging.getLogger()\n",
    "print(f\"Root logger has {len(root_logger.handlers)} handlers:\")\n",
    "for i, handler in enumerate(root_logger.handlers, 1):\n",
    "    print(f\"   {i}. {handler.__class__.__name__} @ level {handler.level}\")\n",
    "    if hasattr(handler, 'baseFilename'):\n",
    "        print(f\"      File: {handler.baseFilename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9c0fa",
   "metadata": {},
   "source": [
    "## Section 8: Testing the Actual Logging System\n",
    "\n",
    "Let's test the real `setup_logging()` function from your project and verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85921e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Now let's use it:\n",
      "\n",
      "Called: logger.warning('CSV file is stale') ONE TIME\n",
      "\n",
      "But it was routed to ALL THREE places:\n",
      "\n",
      "1. Console (operator watching):\n",
      "    'CSV file is stale\\n'\n",
      "\n",
      "2. pipeline.log (developer analyzing):\n",
      "    'CSV file is stale\\n'\n",
      "\n",
      "3. errors.log (on-call engineer triaging):\n",
      "    '' <- Empty because WARNING < ERROR level\n",
      "\n",
      "Key insight: Setup logging ONCE in config.py\n",
      "   Every module.logger.xxx() automatically routes to all handlers!\n"
     ]
    }
   ],
   "source": [
    "# Demo: The magic of adding handlers to root logger\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Clear any existing handlers\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "# Simplifed demo setup\n",
    "console_buffer = io.StringIO()\n",
    "file_buffer = io.StringIO()\n",
    "error_buffer = io.StringIO()\n",
    "\n",
    "console_handler = logging.StreamHandler(console_buffer)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "\n",
    "file_handler = logging.StreamHandler(file_buffer)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "\n",
    "error_handler = logging.StreamHandler(error_buffer)\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "\n",
    "# STEP 1: Add handlers to root logger\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "root_logger.addHandler(console_handler)\n",
    "root_logger.addHandler(file_handler)\n",
    "root_logger.addHandler(error_handler)\n",
    "\n",
    "print(\"Setup complete. Now let's use it:\")\n",
    "print()\n",
    "\n",
    "# STEP 2: In a different module (e.g., etl/extractor.py)\n",
    "# They just do this:\n",
    "logger_from_extractor = logging.getLogger(\"etl.extractor\")\n",
    "\n",
    "# STEP 3: They call logger.warning() ONE TIME\n",
    "logger_from_extractor.warning(\"CSV file is stale\")\n",
    "\n",
    "print(\"Called: logger.warning('CSV file is stale') ONE TIME\")\n",
    "print()\n",
    "print(\"But it was routed to ALL THREE places:\")\n",
    "print()\n",
    "print(\"1. Console (operator watching):\")\n",
    "print(\"   \", repr(console_buffer.getvalue()))\n",
    "print()\n",
    "print(\"2. pipeline.log (developer analyzing):\")\n",
    "print(\"   \", repr(file_buffer.getvalue()))\n",
    "print()\n",
    "print(\"3. errors.log (on-call engineer triaging):\")\n",
    "print(\"   \", repr(error_buffer.getvalue()), \"<- Empty because WARNING < ERROR level\")\n",
    "print()\n",
    "print(\"Key insight: Setup logging ONCE in config.py\")\n",
    "print(\"   Every module.logger.xxx() automatically routes to all handlers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab23359",
   "metadata": {},
   "source": [
    "## Section 7: Attaching All Handlers to Root Logger\n",
    "\n",
    "This is where the magic happens:\n",
    "\n",
    "```python\n",
    "root_logger.addHandler(console_handler)\n",
    "root_logger.addHandler(file_handler)\n",
    "root_logger.addHandler(error_handler)\n",
    "```\n",
    "\n",
    "**What does this do?**\n",
    "\n",
    "Now, when ANY logger in ANY module calls `logger.info(\"message\")`:\n",
    "\n",
    "1. ✅ Goes to console (if level >= INFO)\n",
    "2. ✅ Goes to pipeline.log (if level >= DEBUG)\n",
    "3. ✅ Goes to errors.log (if level >= ERROR)\n",
    "\n",
    "All from ONE `logger.info()` call! No extra work needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5652dfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSOLE OUTPUT (INFO+):\n",
      "INFO     | Successfully extracted 50 rows\n",
      "WARNING  | CSV file is 7 days old - data may be stale\n",
      "ERROR    | API request failed - HTTP 500\n",
      "CRITICAL | Database connection lost - aborting pipeline\n",
      "\n",
      "\n",
      "FILE OUTPUT (DEBUG+):\n",
      "DEBUG    | File size: 3838 bytes\n",
      "INFO     | Successfully extracted 50 rows\n",
      "WARNING  | CSV file is 7 days old - data may be stale\n",
      "ERROR    | API request failed - HTTP 500\n",
      "CRITICAL | Database connection lost - aborting pipeline\n",
      "\n",
      "\n",
      "ERRORS.LOG OUTPUT (ERROR+):\n",
      "ERROR    | API request failed - HTTP 500\n",
      "CRITICAL | Database connection lost - aborting pipeline\n",
      "\n",
      "\n",
      "Same messages, BUT:\n",
      "   - Console: User sees key milestones (less noise)\n",
      "   - File: Developer sees everything (including DEBUG)\n",
      "   - Errors: On-call sees only failures (quick triage)\n"
     ]
    }
   ],
   "source": [
    "# Demo: Three handlers, three views of the same log messages\n",
    "import logging\n",
    "import io\n",
    "\n",
    "# Clear any existing handlers\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "# Create three buffers\n",
    "console_buffer = io.StringIO()\n",
    "file_buffer = io.StringIO()\n",
    "error_buffer = io.StringIO()\n",
    "\n",
    "# Console handler - INFO+\n",
    "console_handler = logging.StreamHandler(console_buffer)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(\"%(levelname)-8s | %(message)s\"))\n",
    "\n",
    "# File handler - DEBUG+\n",
    "file_handler = logging.StreamHandler(file_buffer)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(logging.Formatter(\"%(levelname)-8s | %(message)s\"))\n",
    "\n",
    "# Error handler - ERROR+\n",
    "error_handler = logging.StreamHandler(error_buffer)\n",
    "error_handler.setLevel(logging.ERROR)\n",
    "error_handler.setFormatter(logging.Formatter(\"%(levelname)-8s | %(message)s\"))\n",
    "\n",
    "# Attach all to root logger\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "root_logger.addHandler(console_handler)\n",
    "root_logger.addHandler(file_handler)\n",
    "root_logger.addHandler(error_handler)\n",
    "\n",
    "# Log messages at different levels\n",
    "logger = logging.getLogger(\"etl.extractor\")\n",
    "logger.debug(\"File size: 3838 bytes\")\n",
    "logger.info(\"Successfully extracted 50 rows\")\n",
    "logger.warning(\"CSV file is 7 days old - data may be stale\")\n",
    "logger.error(\"API request failed - HTTP 500\")\n",
    "logger.critical(\"Database connection lost - aborting pipeline\")\n",
    "\n",
    "print(\"CONSOLE OUTPUT (INFO+):\")\n",
    "print(console_buffer.getvalue())\n",
    "print(\"\\nFILE OUTPUT (DEBUG+):\")\n",
    "print(file_buffer.getvalue())\n",
    "print(\"\\nERRORS.LOG OUTPUT (ERROR+):\")\n",
    "print(error_buffer.getvalue())\n",
    "\n",
    "print(\"\\nSame messages, BUT:\")\n",
    "print(\"   - Console: User sees key milestones (less noise)\")\n",
    "print(\"   - File: Developer sees everything (including DEBUG)\")\n",
    "print(\"   - Errors: On-call sees only failures (quick triage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e60522",
   "metadata": {},
   "source": [
    "## Section 6: Setting Up Error-Only Handler\n",
    "\n",
    "This is a **separate** file handler that captures ONLY errors.\n",
    "\n",
    "```python\n",
    "error_handler = RotatingFileHandler(\n",
    "    filename=\"logs/errors.log\",\n",
    "    maxBytes=5 * 1024 * 1024,\n",
    "    backupCount=3\n",
    ")\n",
    "error_handler.setLevel(logging.ERROR)  # ERROR and CRITICAL only\n",
    "```\n",
    "\n",
    "**Why a separate error file?**\n",
    "\n",
    "Imagine `pipeline.log` has 100,000 lines. An on-call engineer wakes up at 3 AM because the pipeline failed. \n",
    "\n",
    "❌ Bad: \"Read all 100k lines\"  \n",
    "✅ Good: \"Read `errors.log` – it has just the 5 failures\"\n",
    "\n",
    "Much faster triage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230d5c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How RotatingFileHandler Works:\n",
      "\n",
      "Scenario: maxBytes=5MB, backupCount=3\n",
      "\n",
      "Initial state:\n",
      "  pipeline.log (0 MB)\n",
      "\n",
      "After pipeline runs (3 MB):\n",
      "  pipeline.log (3 MB)\n",
      "\n",
      "After pipeline runs again (8 MB total written):\n",
      "  pipeline.log (5 MB - just rotated!)\n",
      "  pipeline.log.1 (3 MB - old file moved)\n",
      "\n",
      "After 3 more rotations:\n",
      "  pipeline.log (5 MB)\n",
      "  pipeline.log.1 (5 MB)\n",
      "  pipeline.log.2 (5 MB)\n",
      "  pipeline.log.3 (5 MB)\n",
      "\n",
      "Next rotation (would be 4th):\n",
      "  pipeline.log.3 is DELETED (only keeping 3 backups)\n",
      "  pipeline.log (5 MB)\n",
      "  pipeline.log.1 (5 MB)\n",
      "  pipeline.log.2 (5 MB)\n",
      "\n",
      "Result: Disk space is bounded! Max = 5MB x 4 = 20 MB total\n"
     ]
    }
   ],
   "source": [
    "# Demo: Understanding log rotation\n",
    "import os\n",
    "\n",
    "print(\"How RotatingFileHandler Works:\")\n",
    "print()\n",
    "print(\"Scenario: maxBytes=5MB, backupCount=3\")\n",
    "print()\n",
    "print(\"Initial state:\")\n",
    "print(\"  pipeline.log (0 MB)\")\n",
    "print()\n",
    "print(\"After pipeline runs (3 MB):\")\n",
    "print(\"  pipeline.log (3 MB)\")\n",
    "print()\n",
    "print(\"After pipeline runs again (8 MB total written):\")\n",
    "print(\"  pipeline.log (5 MB - just rotated!)\")\n",
    "print(\"  pipeline.log.1 (3 MB - old file moved)\")\n",
    "print()\n",
    "print(\"After 3 more rotations:\")\n",
    "print(\"  pipeline.log (5 MB)\")\n",
    "print(\"  pipeline.log.1 (5 MB)\")\n",
    "print(\"  pipeline.log.2 (5 MB)\")\n",
    "print(\"  pipeline.log.3 (5 MB)\")\n",
    "print()\n",
    "print(\"Next rotation (would be 4th):\")\n",
    "print(\"  pipeline.log.3 is DELETED (only keeping 3 backups)\")\n",
    "print(\"  pipeline.log (5 MB)\")\n",
    "print(\"  pipeline.log.1 (5 MB)\")\n",
    "print(\"  pipeline.log.2 (5 MB)\")\n",
    "print()\n",
    "print(\"Result: Disk space is bounded! Max = 5MB x 4 = 20 MB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ddd56",
   "metadata": {},
   "source": [
    "## Section 5: Setting Up File Handler with Rotation\n",
    "\n",
    "The **file handler** writes everything to a log file for forensic analysis.\n",
    "\n",
    "**The problem:** Log files can grow infinitely! Your disk fills up. Bad.\n",
    "\n",
    "**The solution:** RotatingFileHandler\n",
    "- Writes to `pipeline.log`\n",
    "- When it hits 5 MB, it rotates: `pipeline.log` → `pipeline.log.1`\n",
    "- Old backups get deleted after 3 rotations\n",
    "- Never fills your disk\n",
    "\n",
    "```python\n",
    "file_handler = RotatingFileHandler(\n",
    "    filename=\"output\\clean_sales.db\",\n",
    "    maxBytes=5 * 1024 * 1024,  # 5 MB\n",
    "    backupCount=3              # Keep 3 old files\n",
    ")\n",
    "file_handler.setLevel(logging.DEBUG)  # Capture EVERYTHING\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5412790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Console handler filtering\n",
    "import logging\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Clear any existing handlers\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "# Create a string buffer to capture output\n",
    "string_buffer = io.StringIO()\n",
    "\n",
    "# Create handlers\n",
    "console_handler = logging.StreamHandler(string_buffer)\n",
    "console_handler.setLevel(logging.INFO)  # INFO and above only\n",
    "\n",
    "# Console formatter\n",
    "console_formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "console_handler.setFormatter(console_formatter)\n",
    "\n",
    "# Setup root logger\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)  # Capture everything\n",
    "root_logger.addHandler(console_handler)\n",
    "\n",
    "# Now log at different levels\n",
    "logger = logging.getLogger(\"demo\")\n",
    "logger.debug(\"This is DEBUG - should NOT appear on console\")\n",
    "logger.info(\"This is INFO - SHOULD appear\")\n",
    "logger.warning(\"This is WARNING - SHOULD appear\")\n",
    "logger.error(\"This is ERROR - SHOULD appear\")\n",
    "\n",
    "# Show what went to console\n",
    "console_output = string_buffer.getvalue()\n",
    "print(\"What appears on the CONSOLE (console level = INFO):\")\n",
    "print(console_output)\n",
    "print()\n",
    "print(\"Notice: DEBUG message was FILTERED OUT\")\n",
    "print(\"   Debug goes to files, not console!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cdac0",
   "metadata": {},
   "source": [
    "## Section 4: Setting Up Console Handler (What Operators See)\n",
    "\n",
    "A **handler** is \"where to send the logs to\". \n",
    "\n",
    "The console handler sends logs to the terminal screen that the operator is watching.\n",
    "\n",
    "```python\n",
    "console_handler = logging.StreamHandler()       # Create handler\n",
    "console_handler.setLevel(logging.INFO)          # Only show INFO+ (not DEBUG)\n",
    "console_handler.setFormatter(console_formatter) # Use simple format\n",
    "```\n",
    "\n",
    "**Why INFO and not DEBUG?**\n",
    "- Operators don't need verbose DEBUG noise while watching in real-time\n",
    "- Too much noise = they miss important warnings\n",
    "- DEBUG is for developers analyzing logs later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Formatters\n",
    "import logging\n",
    "\n",
    "# FILE FORMATTER - detailed, for archival\n",
    "file_formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s | %(levelname)-8s | %(name)-20s | %(filename)s:%(lineno)d | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# CONSOLE FORMATTER - simple, for operators\n",
    "console_formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "print(\"Formatter Template Variables:\")\n",
    "print(\"   %(asctime)s    -> timestamp\")\n",
    "print(\"   %(levelname)s   -> DEBUG, INFO, WARNING, ERROR, CRITICAL\")\n",
    "print(\"   %(name)s        -> logger name (e.g., 'etl.extractor')\")\n",
    "print(\"   %(filename)s    -> source file name\")\n",
    "print(\"   %(lineno)d      -> line number where logging.xxx() was called\")\n",
    "print(\"   %(message)s     -> the actual log message\")\n",
    "print()\n",
    "\n",
    "# Let's create a dummy log record to show the difference\n",
    "handler = logging.StreamHandler()\n",
    "logger = logging.getLogger(\"etl.extractor\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a log record manually for demo\n",
    "record = logging.LogRecord(\n",
    "    name=\"etl.extractor\",\n",
    "    level=logging.INFO,\n",
    "    pathname=\"/path/to/extractor.py\",\n",
    "    lineno=69,\n",
    "    msg=\"Successfully extracted 50 rows\",\n",
    "    args=(),\n",
    "    exc_info=None\n",
    ")\n",
    "\n",
    "print(\"Same log message, different formatters:\")\n",
    "print()\n",
    "print(\"FILE output:\")\n",
    "print(\"  \", file_formatter.format(record))\n",
    "print()\n",
    "print(\"CONSOLE output:\")\n",
    "print(\"  \", console_formatter.format(record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea4fbf",
   "metadata": {},
   "source": [
    "## Section 3: Defining Custom Formatters\n",
    "\n",
    "A **formatter** controls HOW a log message looks. Think of it as a template.\n",
    "\n",
    "The code creates **two different formatters**:\n",
    "\n",
    "**1. File Formatter (detailed):**\n",
    "```\n",
    "2026-02-10 21:44:02 | INFO     | etl.extractor | extractor.py:69 | Successfully extracted 50 rows\n",
    "```\n",
    "Components: `timestamp | level | logger_name | filename:line | message`\n",
    "\n",
    "**2. Console Formatter (simple):**\n",
    "```\n",
    "21:44:02 | INFO     | Successfully extracted 50 rows\n",
    "```\n",
    "Components: `time | level | message` (no filename/logger because operators don't need it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02640746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Root logger vs named loggers\n",
    "import logging\n",
    "\n",
    "# Clear existing handlers for demo\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "# Step 1: Configure the ROOT logger (happens once)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"Root logger setup:\")\n",
    "print(f\"   - Root logger name: '{root_logger.name}' (empty = root)\")\n",
    "print(f\"   - Root logger level: {root_logger.level} (DEBUG)\")\n",
    "print()\n",
    "\n",
    "# Step 2: Create named loggers in different modules\n",
    "# These inherit root logger's settings\n",
    "logger_extractor = logging.getLogger(\"etl.extractor\")\n",
    "logger_transformer = logging.getLogger(\"etl.transformer\")\n",
    "logger_loader = logging.getLogger(\"etl.loader\")\n",
    "\n",
    "print(\"Named loggers created:\")\n",
    "print(f\"   - logger_extractor name: '{logger_extractor.name}'\")\n",
    "print(f\"   - logger_transformer name: '{logger_transformer.name}'\")\n",
    "print(f\"   - logger_loader name: '{logger_loader.name}'\")\n",
    "print()\n",
    "\n",
    "print(\"Key point:\")\n",
    "print(\"   All three inherited the root logger's level (DEBUG)\")\n",
    "print(\"   We set it once, everyone gets it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d902068",
   "metadata": {},
   "source": [
    "## Section 2: Creating the Root Logger\n",
    "\n",
    "The **root logger** is like the CEO of all loggers. Every logger you create inherits its settings.\n",
    "\n",
    "```python\n",
    "root_logger = logging.getLogger()  # No name = root logger\n",
    "root_logger.setLevel(logging.DEBUG)  # Capture EVERYTHING\n",
    "root_logger.handlers.clear()  # Wipe any old handlers\n",
    "```\n",
    "\n",
    "**Why do this?**\n",
    "- You configure ONCE here\n",
    "- Every module that does `logger = logging.getLogger(__name__)` automatically inherits these settings\n",
    "- Prevents logging configuration chaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the severity order in Python\n",
    "import logging\n",
    "\n",
    "levels = {\n",
    "    'DEBUG': logging.DEBUG,\n",
    "    'INFO': logging.INFO,\n",
    "    'WARNING': logging.WARNING,\n",
    "    'ERROR': logging.ERROR,\n",
    "    'CRITICAL': logging.CRITICAL\n",
    "}\n",
    "\n",
    "for name, value in levels.items():\n",
    "    print(f\"{name:10} = {value}\")\n",
    "\n",
    "print(\"\\nRemember: Higher numbers = More severe\")\n",
    "print(\"   If you set level to INFO (20), you accept: INFO, WARNING, ERROR, CRITICAL\")\n",
    "print(\"   But you REJECT: DEBUG (10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632ba2b",
   "metadata": {},
   "source": [
    "## Section 1: Understanding Logging Levels\n",
    "\n",
    "Python has **5 severity levels** for logging. Think of them like priority signals:\n",
    "\n",
    "| Level | Severity | When to Use | Example |\n",
    "|-------|----------|------------|---------|\n",
    "| DEBUG | Low | Internal workings you need as a developer | \"File size: 3838 bytes\" |\n",
    "| INFO | Low-Medium | Important milestones, everything is OK | \"Successfully extracted 50 rows\" |\n",
    "| WARNING | Medium | Something odd happened, but we fixed it | \"Missing email field – filling with placeholder\" |\n",
    "| ERROR | High | Something failed for this specific item | \"Invalid quantity type – skipping row\" |\n",
    "| CRITICAL | Very High | The whole pipeline must stop | \"Database connection failed – aborting\" |\n",
    "\n",
    "**Key insight:** Each level includes all higher levels. So if you set `INFO`, you get INFO + WARNING + ERROR + CRITICAL, but NOT DEBUG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce1f4b",
   "metadata": {},
   "source": [
    "# Understanding Python Logging Configuration\n",
    "## A Deep Dive into the ETL Pipeline's Logging Setup\n",
    "\n",
    "This notebook explains **exactly** what the `logging_config.py` code does, step by step, with interactive examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
